{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8c985ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/anaconda3/lib/python3.11/site-packages (2.17.0)\n",
      "Requirement already satisfied: keras in /opt/anaconda3/lib/python3.11/site-packages (3.5.0)\n",
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.66.2)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.17.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.11/site-packages (from keras) (13.3.5)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.11/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.11/site-packages (from keras) (0.12.1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich->keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich->keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras) (0.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow keras nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73f29fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "import random\n",
    "import nltk\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "word_normalizer = WordNetLemmatizer()\n",
    "\n",
    "import json\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "chat_parsed_intents_file = open('parsed_intents.json').read()\n",
    "\n",
    "parsed_intents = json.loads(chat_parsed_intents_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "972b9a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/karan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "013353cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['Hi', 'there'], 'greeting'), (['How', 'are', 'you'], 'greeting'), (['Is', 'anyone', 'there', '?'], 'greeting'), (['Hey'], 'greeting'), (['Hola'], 'greeting'), (['Hello'], 'greeting'), (['Good', 'day'], 'greeting'), (['Bye'], 'goodbye'), (['See', 'you', 'later'], 'goodbye'), (['Goodbye'], 'goodbye'), (['Nice', 'chatting', 'to', 'you', ',', 'bye'], 'goodbye'), (['Till', 'next', 'time'], 'goodbye'), (['Thanks'], 'thanks'), (['Thank', 'you'], 'thanks'), (['That', \"'s\", 'helpful'], 'thanks'), (['Awesome', ',', 'thanks'], 'thanks'), (['Thanks', 'for', 'helping', 'me'], 'thanks'), (['How', 'you', 'could', 'help', 'me', '?'], 'options'), (['What', 'you', 'can', 'do', '?'], 'options'), (['What', 'help', 'you', 'provide', '?'], 'options'), (['How', 'you', 'can', 'be', 'helpful', '?'], 'options'), (['What', 'support', 'is', 'offered'], 'options'), (['How', 'to', 'check', 'Adverse', 'drug', 'reaction', '?'], 'adverse_drug'), (['Open', 'adverse', 'drugs', 'module'], 'adverse_drug'), (['Give', 'me', 'a', 'list', 'of', 'drugs', 'causing', 'adverse', 'behavior'], 'adverse_drug'), (['List', 'all', 'drugs', 'suitable', 'for', 'patient', 'with', 'adverse', 'reaction'], 'adverse_drug'), (['Which', 'drugs', 'dont', 'have', 'adverse', 'reaction', '?'], 'adverse_drug'), (['Open', 'blood', 'pressure', 'module'], 'blood_pressure'), (['Task', 'related', 'to', 'blood', 'pressure'], 'blood_pressure'), (['Blood', 'pressure', 'data', 'entry'], 'blood_pressure'), (['I', 'want', 'to', 'log', 'blood', 'pressure', 'results'], 'blood_pressure'), (['Blood', 'pressure', 'data', 'management'], 'blood_pressure'), (['I', 'want', 'to', 'search', 'for', 'blood', 'pressure', 'result', 'history'], 'blood_pressure_search'), (['Blood', 'pressure', 'for', 'patient'], 'blood_pressure_search'), (['Load', 'patient', 'blood', 'pressure', 'result'], 'blood_pressure_search'), (['Show', 'blood', 'pressure', 'results', 'for', 'patient'], 'blood_pressure_search'), (['Find', 'blood', 'pressure', 'results', 'by', 'ID'], 'blood_pressure_search'), (['Find', 'me', 'a', 'pharmacy'], 'pharmacy_search'), (['Find', 'pharmacy'], 'pharmacy_search'), (['List', 'of', 'pharmacies', 'nearby'], 'pharmacy_search'), (['Locate', 'pharmacy'], 'pharmacy_search'), (['Search', 'pharmacy'], 'pharmacy_search'), (['Lookup', 'for', 'hospital'], 'hospital_search'), (['Searching', 'for', 'hospital', 'to', 'transfer', 'patient'], 'hospital_search'), (['I', 'want', 'to', 'search', 'hospital', 'data'], 'hospital_search'), (['Hospital', 'lookup', 'for', 'patient'], 'hospital_search'), (['Looking', 'up', 'hospital', 'details'], 'hospital_search')]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_list=[]\n",
    "\n",
    "intent_categories = []\n",
    "\n",
    "input_corpus = []\n",
    "\n",
    "punctuation_marks = ['!', '?', ',', '.']\n",
    "\n",
    "\n",
    "for intent in parsed_intents['parsed_intents']:\n",
    "\n",
    "    for pattern in intent['patterns']:\n",
    "\n",
    "        # Break down each sentence into individual tokens\n",
    "\n",
    "        word = nltk.word_tokenize(pattern)\n",
    "\n",
    "        vocabulary_list.extend(word)        \n",
    "\n",
    "        #add input_corpus in the corpus\n",
    "\n",
    "        input_corpus.append((word, intent['tag']))\n",
    "\n",
    "        # add to our intent_categories list\n",
    "\n",
    "        if intent['tag'] not in intent_categories:\n",
    "\n",
    "            intent_categories.append(intent['tag'])\n",
    "\n",
    "\n",
    "print(input_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02f69e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/karan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b30529ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 input_corpus\n",
      "9 intent_categories ['adverse_drug', 'blood_pressure', 'blood_pressure_search', 'goodbye', 'greeting', 'hospital_search', 'options', 'pharmacy_search', 'thanks']\n",
      "87 unique lemmatized vocabulary_list [\"'s\", 'a', 'adverse', 'all', 'anyone', 'are', 'awesome', 'be', 'behavior', 'blood', 'by', 'bye', 'can', 'causing', 'chatting', 'check', 'could', 'data', 'day', 'detail', 'do', 'dont', 'drug', 'entry', 'find', 'for', 'give', 'good', 'goodbye', 'have', 'hello', 'help', 'helpful', 'helping', 'hey', 'hi', 'history', 'hola', 'hospital', 'how', 'i', 'id', 'is', 'later', 'list', 'load', 'locate', 'log', 'looking', 'lookup', 'management', 'me', 'module', 'nearby', 'next', 'nice', 'of', 'offered', 'open', 'patient', 'pharmacy', 'pressure', 'provide', 'reaction', 'related', 'result', 'search', 'searching', 'see', 'show', 'suitable', 'support', 'task', 'thank', 'thanks', 'that', 'there', 'till', 'time', 'to', 'transfer', 'up', 'want', 'what', 'which', 'with', 'you']\n"
     ]
    }
   ],
   "source": [
    "# Normalize, lowercase, and remove duplicates from the word list\n",
    "\n",
    "vocabulary_list = [word_normalizer.lemmatize(w.lower()) for w in vocabulary_list if w not in punctuation_marks]\n",
    "\n",
    "vocabulary_list = sorted(list(set(vocabulary_list)))\n",
    "\n",
    "# sort intent_categories\n",
    "\n",
    "intent_categories = sorted(list(set(intent_categories)))\n",
    "\n",
    "# input_corpus = combination between patterns and parsed_intents\n",
    "\n",
    "print (len(input_corpus), \"input_corpus\")\n",
    "\n",
    "# intent_categories = parsed_intents\n",
    "\n",
    "print (len(intent_categories), \"intent_categories\", intent_categories)\n",
    "\n",
    "# vocabulary_list = all vocabulary_list, vocabulary\n",
    "\n",
    "print (len(vocabulary_list), \"unique lemmatized vocabulary_list\", vocabulary_list)\n",
    "\n",
    "\n",
    "pickle.dump(vocabulary_list,open('vocabulary_list.pkl','wb'))\n",
    "\n",
    "pickle.dump(intent_categories,open('intent_categories.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "beade50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data is created\n"
     ]
    }
   ],
   "source": [
    "training_data = []\n",
    "\n",
    "# Create empty array for the output\n",
    "initial_output = [0] * len(intent_categories)\n",
    "\n",
    "# Training set, word_presence_bag of vocabulary_list for every sentence\n",
    "for doc in input_corpus:\n",
    "\n",
    "    # Initializing word_presence_bag of vocabulary_list\n",
    "    word_presence_bag = []\n",
    "\n",
    "    # List of tokenized vocabulary_list for the pattern\n",
    "    word_patterns = doc[0]\n",
    "\n",
    "    # Lemmatize each word - create base word, in attempt to represent related vocabulary_list\n",
    "    word_patterns = [word_normalizer.lemmatize(word.lower()) for word in word_patterns]\n",
    "\n",
    "    # Create the word_presence_bag of vocabulary_list array with 1 if the word is found in current pattern\n",
    "    for word in vocabulary_list:\n",
    "        word_presence_bag.append(1 if word in word_patterns else 0)\n",
    "\n",
    "    # Output is a '0' for each tag and '1' for current tag (for each pattern)\n",
    "    output_row = list(initial_output)  # Copy the empty array\n",
    "    output_row[intent_categories.index(doc[1])] = 1\n",
    "\n",
    "    # Append both the word_presence_bag of vocabulary_list and the corresponding one-hot encoded class to the training_data data\n",
    "    training_data.append([word_presence_bag, output_row])\n",
    "\n",
    "# Convert training_data data to a NumPy array\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Shuffle the features and make numpy array\n",
    "random.shuffle(training_data)\n",
    "\n",
    "# Separate the data into X (features) and Y (labels)\n",
    "train_x = np.array([item[0] for item in training_data])  # Extract word_presence_bags of vocabulary_list\n",
    "train_y = np.array([item[1] for item in training_data])  # Extract one-hot encoded outputs\n",
    "\n",
    "print(\"Training data is created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "207b21b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py:33: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 626us/step - accuracy: 0.1332 - loss: 2.2741\n",
      "Epoch 2/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - accuracy: 0.2203 - loss: 2.1138\n",
      "Epoch 3/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 509us/step - accuracy: 0.1806 - loss: 2.1288  \n",
      "Epoch 4/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 483us/step - accuracy: 0.3194 - loss: 1.9799\n",
      "Epoch 5/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466us/step - accuracy: 0.3625 - loss: 1.9462\n",
      "Epoch 6/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 484us/step - accuracy: 0.5126 - loss: 1.8113\n",
      "Epoch 7/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - accuracy: 0.4812 - loss: 1.7686\n",
      "Epoch 8/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - accuracy: 0.4642 - loss: 1.6574\n",
      "Epoch 9/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step - accuracy: 0.6451 - loss: 1.3462\n",
      "Epoch 10/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 482us/step - accuracy: 0.6009 - loss: 1.4986\n",
      "Epoch 11/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 479us/step - accuracy: 0.4946 - loss: 1.3822\n",
      "Epoch 12/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 482us/step - accuracy: 0.7535 - loss: 1.1278\n",
      "Epoch 13/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 605us/step - accuracy: 0.7599 - loss: 0.9613\n",
      "Epoch 14/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643us/step - accuracy: 0.8265 - loss: 0.8092\n",
      "Epoch 15/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - accuracy: 0.7624 - loss: 0.8237\n",
      "Epoch 16/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - accuracy: 0.8426 - loss: 0.6379\n",
      "Epoch 17/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 479us/step - accuracy: 0.9397 - loss: 0.6574\n",
      "Epoch 18/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 451us/step - accuracy: 0.8776 - loss: 0.6464\n",
      "Epoch 19/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 485us/step - accuracy: 0.9473 - loss: 0.5214\n",
      "Epoch 20/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 454us/step - accuracy: 0.8635 - loss: 0.4485\n",
      "Epoch 21/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 457us/step - accuracy: 0.9112 - loss: 0.3964\n",
      "Epoch 22/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - accuracy: 0.8318 - loss: 0.5096\n",
      "Epoch 23/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 630us/step - accuracy: 0.9443 - loss: 0.3317\n",
      "Epoch 24/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 566us/step - accuracy: 0.9133 - loss: 0.3008\n",
      "Epoch 25/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - accuracy: 0.8485 - loss: 0.4703\n",
      "Epoch 26/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 501us/step - accuracy: 0.9332 - loss: 0.3434\n",
      "Epoch 27/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456us/step - accuracy: 0.9440 - loss: 0.3327\n",
      "Epoch 28/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 463us/step - accuracy: 0.9500 - loss: 0.2424\n",
      "Epoch 29/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466us/step - accuracy: 0.8841 - loss: 0.2798\n",
      "Epoch 30/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - accuracy: 0.8697 - loss: 0.2941\n",
      "Epoch 31/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 602us/step - accuracy: 0.9447 - loss: 0.2536\n",
      "Epoch 32/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 728us/step - accuracy: 0.9214 - loss: 0.1912\n",
      "Epoch 33/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step - accuracy: 0.9720 - loss: 0.2358\n",
      "Epoch 34/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 472us/step - accuracy: 1.0000 - loss: 0.2041\n",
      "Epoch 35/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 514us/step - accuracy: 0.9661 - loss: 0.2548\n",
      "Epoch 36/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 494us/step - accuracy: 0.9570 - loss: 0.1203\n",
      "Epoch 37/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 598us/step - accuracy: 0.9531 - loss: 0.1467\n",
      "Epoch 38/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - accuracy: 1.0000 - loss: 0.1997\n",
      "Epoch 39/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step - accuracy: 0.9341 - loss: 0.2229\n",
      "Epoch 40/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 470us/step - accuracy: 0.9220 - loss: 0.1653\n",
      "Epoch 41/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 477us/step - accuracy: 1.0000 - loss: 0.0913\n",
      "Epoch 42/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 463us/step - accuracy: 0.9612 - loss: 0.1602\n",
      "Epoch 43/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 459us/step - accuracy: 0.9339 - loss: 0.2198\n",
      "Epoch 44/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 571us/step - accuracy: 0.9364 - loss: 0.1364\n",
      "Epoch 45/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 672us/step - accuracy: 0.9882 - loss: 0.1187\n",
      "Epoch 46/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 475us/step - accuracy: 0.9661 - loss: 0.1603\n",
      "Epoch 47/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 469us/step - accuracy: 1.0000 - loss: 0.1178\n",
      "Epoch 48/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 473us/step - accuracy: 0.9561 - loss: 0.1689\n",
      "Epoch 49/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 468us/step - accuracy: 1.0000 - loss: 0.1555\n",
      "Epoch 50/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 446us/step - accuracy: 0.9567 - loss: 0.1843\n",
      "Epoch 51/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 513us/step - accuracy: 0.9720 - loss: 0.1292\n",
      "Epoch 52/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540us/step - accuracy: 1.0000 - loss: 0.0271\n",
      "Epoch 53/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554us/step - accuracy: 0.9651 - loss: 0.1162\n",
      "Epoch 54/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - accuracy: 1.0000 - loss: 0.0537\n",
      "Epoch 55/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 493us/step - accuracy: 0.9584 - loss: 0.1261\n",
      "Epoch 56/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 450us/step - accuracy: 0.9862 - loss: 0.0871\n",
      "Epoch 57/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step - accuracy: 1.0000 - loss: 0.1216\n",
      "Epoch 58/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 453us/step - accuracy: 0.9941 - loss: 0.0956\n",
      "Epoch 59/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 453us/step - accuracy: 0.9455 - loss: 0.1162\n",
      "Epoch 60/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 451us/step - accuracy: 1.0000 - loss: 0.0901\n",
      "Epoch 61/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546us/step - accuracy: 0.9826 - loss: 0.0468\n",
      "Epoch 62/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 573us/step - accuracy: 0.8928 - loss: 0.2188\n",
      "Epoch 63/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554us/step - accuracy: 0.9803 - loss: 0.0945\n",
      "Epoch 64/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 630us/step - accuracy: 1.0000 - loss: 0.0407\n",
      "Epoch 65/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 480us/step - accuracy: 1.0000 - loss: 0.0208\n",
      "Epoch 66/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 470us/step - accuracy: 0.9720 - loss: 0.0545\n",
      "Epoch 67/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 462us/step - accuracy: 0.9941 - loss: 0.0637\n",
      "Epoch 68/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467us/step - accuracy: 1.0000 - loss: 0.0290\n",
      "Epoch 69/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 461us/step - accuracy: 0.9826 - loss: 0.0818\n",
      "Epoch 70/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 480us/step - accuracy: 1.0000 - loss: 0.0222  \n",
      "Epoch 71/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 454us/step - accuracy: 1.0000 - loss: 0.0739\n",
      "Epoch 72/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 444us/step - accuracy: 1.0000 - loss: 0.0252\n",
      "Epoch 73/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 479us/step - accuracy: 1.0000 - loss: 0.0368\n",
      "Epoch 74/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 570us/step - accuracy: 1.0000 - loss: 0.0514\n",
      "Epoch 75/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step - accuracy: 0.9918 - loss: 0.0443\n",
      "Epoch 76/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554us/step - accuracy: 0.9941 - loss: 0.0437\n",
      "Epoch 77/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - accuracy: 1.0000 - loss: 0.0318\n",
      "Epoch 78/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 587us/step - accuracy: 0.9941 - loss: 0.0375\n",
      "Epoch 79/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 567us/step - accuracy: 0.9447 - loss: 0.2152\n",
      "Epoch 80/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 505us/step - accuracy: 0.9961 - loss: 0.0318\n",
      "Epoch 81/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 465us/step - accuracy: 1.0000 - loss: 0.0468\n",
      "Epoch 82/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 457us/step - accuracy: 0.9629 - loss: 0.1447\n",
      "Epoch 83/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 471us/step - accuracy: 0.9780 - loss: 0.1185\n",
      "Epoch 84/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 465us/step - accuracy: 1.0000 - loss: 0.0424\n",
      "Epoch 85/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 468us/step - accuracy: 1.0000 - loss: 0.0520\n",
      "Epoch 86/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456us/step - accuracy: 1.0000 - loss: 0.0677\n",
      "Epoch 87/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 460us/step - accuracy: 1.0000 - loss: 0.0643\n",
      "Epoch 88/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436us/step - accuracy: 1.0000 - loss: 0.0462\n",
      "Epoch 89/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 470us/step - accuracy: 1.0000 - loss: 0.0475\n",
      "Epoch 90/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 451us/step - accuracy: 1.0000 - loss: 0.0238  \n",
      "Epoch 91/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step - accuracy: 0.9941 - loss: 0.0299\n",
      "Epoch 92/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step - accuracy: 1.0000 - loss: 0.0375\n",
      "Epoch 93/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step - accuracy: 0.9720 - loss: 0.0632\n",
      "Epoch 94/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 617us/step - accuracy: 1.0000 - loss: 0.0433\n",
      "Epoch 95/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 568us/step - accuracy: 1.0000 - loss: 0.0315\n",
      "Epoch 96/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 498us/step - accuracy: 1.0000 - loss: 0.0141\n",
      "Epoch 97/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 465us/step - accuracy: 0.9780 - loss: 0.0373\n",
      "Epoch 98/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 469us/step - accuracy: 1.0000 - loss: 0.0250\n",
      "Epoch 99/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 451us/step - accuracy: 1.0000 - loss: 0.0255\n",
      "Epoch 100/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 460us/step - accuracy: 1.0000 - loss: 0.0129\n",
      "Epoch 101/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 451us/step - accuracy: 1.0000 - loss: 0.0216\n",
      "Epoch 102/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 474us/step - accuracy: 1.0000 - loss: 0.0093\n",
      "Epoch 103/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467us/step - accuracy: 0.9720 - loss: 0.0355  \n",
      "Epoch 104/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541us/step - accuracy: 1.0000 - loss: 0.0581\n",
      "Epoch 105/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - accuracy: 1.0000 - loss: 0.0198\n",
      "Epoch 106/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - accuracy: 0.9826 - loss: 0.0817\n",
      "Epoch 107/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 595us/step - accuracy: 0.9629 - loss: 0.0639\n",
      "Epoch 108/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 621us/step - accuracy: 0.9638 - loss: 0.0472\n",
      "Epoch 109/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 489us/step - accuracy: 1.0000 - loss: 0.0288\n",
      "Epoch 110/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 492us/step - accuracy: 1.0000 - loss: 0.0066\n",
      "Epoch 111/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 472us/step - accuracy: 1.0000 - loss: 0.0271\n",
      "Epoch 112/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 442us/step - accuracy: 0.9661 - loss: 0.0794\n",
      "Epoch 113/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 449us/step - accuracy: 0.9780 - loss: 0.0885\n",
      "Epoch 114/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 452us/step - accuracy: 1.0000 - loss: 0.0146\n",
      "Epoch 115/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 447us/step - accuracy: 1.0000 - loss: 0.0079\n",
      "Epoch 116/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 455us/step - accuracy: 1.0000 - loss: 0.0255\n",
      "Epoch 117/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - accuracy: 1.0000 - loss: 0.0159\n",
      "Epoch 118/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 563us/step - accuracy: 1.0000 - loss: 0.0121  \n",
      "Epoch 119/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 612us/step - accuracy: 0.9918 - loss: 0.0172\n",
      "Epoch 120/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 559us/step - accuracy: 1.0000 - loss: 0.0367\n",
      "Epoch 121/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 481us/step - accuracy: 0.9720 - loss: 0.0912\n",
      "Epoch 122/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456us/step - accuracy: 1.0000 - loss: 0.0182\n",
      "Epoch 123/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464us/step - accuracy: 0.9780 - loss: 0.0482\n",
      "Epoch 124/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 477us/step - accuracy: 1.0000 - loss: 0.0684  \n",
      "Epoch 125/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467us/step - accuracy: 0.9365 - loss: 0.0823\n",
      "Epoch 126/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 468us/step - accuracy: 0.9961 - loss: 0.0212\n",
      "Epoch 127/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 487us/step - accuracy: 0.9862 - loss: 0.0961\n",
      "Epoch 128/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456us/step - accuracy: 0.9918 - loss: 0.0371\n",
      "Epoch 129/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step - accuracy: 0.9724 - loss: 0.0634\n",
      "Epoch 130/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 536us/step - accuracy: 0.9961 - loss: 0.0343\n",
      "Epoch 131/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step - accuracy: 0.9447 - loss: 0.0976\n",
      "Epoch 132/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - accuracy: 0.9826 - loss: 0.0289\n",
      "Epoch 133/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 631us/step - accuracy: 1.0000 - loss: 0.0546\n",
      "Epoch 134/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 469us/step - accuracy: 1.0000 - loss: 0.0239\n",
      "Epoch 135/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466us/step - accuracy: 1.0000 - loss: 0.0059\n",
      "Epoch 136/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 450us/step - accuracy: 0.9826 - loss: 0.0280\n",
      "Epoch 137/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 459us/step - accuracy: 0.9780 - loss: 0.0292\n",
      "Epoch 138/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 474us/step - accuracy: 1.0000 - loss: 0.0629\n",
      "Epoch 139/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 463us/step - accuracy: 1.0000 - loss: 0.0237\n",
      "Epoch 140/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 480us/step - accuracy: 1.0000 - loss: 0.0293\n",
      "Epoch 141/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 500us/step - accuracy: 0.9780 - loss: 0.0619\n",
      "Epoch 142/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 468us/step - accuracy: 0.9918 - loss: 0.0244\n",
      "Epoch 143/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 572us/step - accuracy: 1.0000 - loss: 0.0331\n",
      "Epoch 144/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step - accuracy: 1.0000 - loss: 0.0142\n",
      "Epoch 145/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 590us/step - accuracy: 0.9892 - loss: 0.0386\n",
      "Epoch 146/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - accuracy: 1.0000 - loss: 0.0162\n",
      "Epoch 147/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 484us/step - accuracy: 1.0000 - loss: 0.0104\n",
      "Epoch 148/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467us/step - accuracy: 1.0000 - loss: 0.0078\n",
      "Epoch 149/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466us/step - accuracy: 1.0000 - loss: 0.0176  \n",
      "Epoch 150/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 468us/step - accuracy: 1.0000 - loss: 0.0454\n",
      "Epoch 151/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 463us/step - accuracy: 1.0000 - loss: 0.0137  \n",
      "Epoch 152/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466us/step - accuracy: 0.9826 - loss: 0.0232\n",
      "Epoch 153/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 454us/step - accuracy: 0.9673 - loss: 0.0751\n",
      "Epoch 154/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 624us/step - accuracy: 0.9941 - loss: 0.0111\n",
      "Epoch 155/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 507us/step - accuracy: 1.0000 - loss: 0.0066\n",
      "Epoch 156/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - accuracy: 1.0000 - loss: 0.0153\n",
      "Epoch 157/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 516us/step - accuracy: 1.0000 - loss: 0.0142\n",
      "Epoch 158/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541us/step - accuracy: 1.0000 - loss: 0.0102\n",
      "Epoch 159/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step - accuracy: 0.9918 - loss: 0.0286\n",
      "Epoch 160/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 568us/step - accuracy: 0.9892 - loss: 0.0470\n",
      "Epoch 161/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step - accuracy: 1.0000 - loss: 0.0056\n",
      "Epoch 162/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 507us/step - accuracy: 0.9673 - loss: 0.0671  \n",
      "Epoch 163/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520us/step - accuracy: 1.0000 - loss: 0.0032\n",
      "Epoch 164/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 446us/step - accuracy: 0.9862 - loss: 0.0515\n",
      "Epoch 165/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 474us/step - accuracy: 1.0000 - loss: 0.0052\n",
      "Epoch 166/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 452us/step - accuracy: 0.9629 - loss: 0.0451\n",
      "Epoch 167/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 478us/step - accuracy: 1.0000 - loss: 0.0442  \n",
      "Epoch 168/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 469us/step - accuracy: 1.0000 - loss: 0.0153\n",
      "Epoch 169/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 455us/step - accuracy: 1.0000 - loss: 0.0109\n",
      "Epoch 170/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 463us/step - accuracy: 1.0000 - loss: 0.0182\n",
      "Epoch 171/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464us/step - accuracy: 1.0000 - loss: 0.0116\n",
      "Epoch 172/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 460us/step - accuracy: 1.0000 - loss: 0.0099\n",
      "Epoch 173/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528us/step - accuracy: 0.9918 - loss: 0.0159\n",
      "Epoch 174/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 505us/step - accuracy: 0.9721 - loss: 0.0506\n",
      "Epoch 175/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 563us/step - accuracy: 1.0000 - loss: 0.0188  \n",
      "Epoch 176/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step - accuracy: 0.9918 - loss: 0.0191\n",
      "Epoch 177/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - accuracy: 1.0000 - loss: 0.0182\n",
      "Epoch 178/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540us/step - accuracy: 1.0000 - loss: 0.0111\n",
      "Epoch 179/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 473us/step - accuracy: 0.9388 - loss: 0.0902\n",
      "Epoch 180/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 449us/step - accuracy: 1.0000 - loss: 0.0041\n",
      "Epoch 181/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 463us/step - accuracy: 1.0000 - loss: 0.0242\n",
      "Epoch 182/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 444us/step - accuracy: 0.9918 - loss: 0.0297\n",
      "Epoch 183/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 457us/step - accuracy: 1.0000 - loss: 0.0227\n",
      "Epoch 184/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 451us/step - accuracy: 1.0000 - loss: 0.0124\n",
      "Epoch 185/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456us/step - accuracy: 1.0000 - loss: 0.0065\n",
      "Epoch 186/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 471us/step - accuracy: 1.0000 - loss: 0.0228\n",
      "Epoch 187/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 468us/step - accuracy: 1.0000 - loss: 0.0557\n",
      "Epoch 188/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 461us/step - accuracy: 1.0000 - loss: 0.0116  \n",
      "Epoch 189/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 510us/step - accuracy: 1.0000 - loss: 0.0142\n",
      "Epoch 190/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 505us/step - accuracy: 1.0000 - loss: 0.0136\n",
      "Epoch 191/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546us/step - accuracy: 1.0000 - loss: 0.0095  \n",
      "Epoch 192/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step - accuracy: 1.0000 - loss: 0.0020\n",
      "Epoch 193/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557us/step - accuracy: 1.0000 - loss: 0.0064  \n",
      "Epoch 194/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 639us/step - accuracy: 1.0000 - loss: 0.0093  \n",
      "Epoch 195/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 479us/step - accuracy: 1.0000 - loss: 0.0286\n",
      "Epoch 196/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 455us/step - accuracy: 1.0000 - loss: 0.0194\n",
      "Epoch 197/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464us/step - accuracy: 0.9780 - loss: 0.0323\n",
      "Epoch 198/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 460us/step - accuracy: 1.0000 - loss: 0.0066\n",
      "Epoch 199/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 470us/step - accuracy: 1.0000 - loss: 0.0045\n",
      "Epoch 200/200\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 462us/step - accuracy: 1.0000 - loss: 0.0135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is created\n"
     ]
    }
   ],
   "source": [
    "# deep neural netvocabulary_list model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
    "\n",
    "\n",
    "# Compiling model. SGD with Nesterov accelerated gradient gives good results for this model\n",
    "\n",
    "sgd = SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#Training and saving the model \n",
    "\n",
    "hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
    "\n",
    "model.save('chatbot_model.h5', hist)\n",
    "\n",
    "\n",
    "print(\"model is created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "819b7b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "word_normalizer = WordNetLemmatizer()\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('chatbot_model.h5')\n",
    "\n",
    "import json\n",
    "\n",
    "import random\n",
    "\n",
    "parsed_intents = json.loads(open('parsed_intents.json').read())\n",
    "\n",
    "vocabulary_list = pickle.load(open('vocabulary_list.pkl','rb'))\n",
    "\n",
    "intent_categories = pickle.load(open('intent_categories.pkl','rb'))\n",
    "\n",
    "\n",
    "def clean_up_sentence(sentence):\n",
    "\n",
    "    # tokenize the pattern - splitting vocabulary_list into array\n",
    "\n",
    "    sentence_vocabulary_list = nltk.word_tokenize(sentence)\n",
    "\n",
    "    # stemming every word - reducing to base form\n",
    "\n",
    "    sentence_vocabulary_list = [word_normalizer.lemmatize(word.lower()) for word in sentence_vocabulary_list]\n",
    "\n",
    "    return sentence_vocabulary_list\n",
    "\n",
    "# return word_presence_bag of vocabulary_list array: 0 or 1 for vocabulary_list that exist in sentence\n",
    "\n",
    "def word_presence_bag_of_vocabulary_list(sentence, vocabulary_list, show_details=True):\n",
    "\n",
    "    # tokenizing patterns\n",
    "\n",
    "    sentence_vocabulary_list = clean_up_sentence(sentence)\n",
    "\n",
    "    # word_presence_bag of vocabulary_list - vocabulary matrix\n",
    "\n",
    "    word_presence_bag = [0]*len(vocabulary_list)  \n",
    "\n",
    "    for s in sentence_vocabulary_list:\n",
    "\n",
    "        for i,word in enumerate(vocabulary_list):\n",
    "\n",
    "            if word == s: \n",
    "\n",
    "                # assign 1 if current word is in the vocabulary position\n",
    "\n",
    "                word_presence_bag[i] = 1\n",
    "\n",
    "                if show_details:\n",
    "\n",
    "                    print (\"found in word_presence_bag: %s\" % word)\n",
    "\n",
    "    return(np.array(word_presence_bag))\n",
    "\n",
    "\n",
    "def predict_class(sentence):\n",
    "\n",
    "    # filter below  threshold predictions\n",
    "\n",
    "    p = word_presence_bag_of_vocabulary_list(sentence, vocabulary_list,show_details=False)\n",
    "\n",
    "    res = model.predict(np.array([p]))[0]\n",
    "\n",
    "    ERROR_THRESHOLD = 0.25\n",
    "\n",
    "    results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\n",
    "\n",
    "    # sorting strength probability\n",
    "\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "\n",
    "    for r in results:\n",
    "\n",
    "        return_list.append({\"intent\": intent_categories[r[0]], \"probability\": str(r[1])})\n",
    "\n",
    "    return return_list\n",
    "\n",
    "\n",
    "def getResponse(ints, parsed_intents_json):\n",
    "\n",
    "    tag = ints[0]['intent']\n",
    "\n",
    "    list_of_parsed_intents = parsed_intents_json['parsed_intents']\n",
    "\n",
    "    for i in list_of_parsed_intents:\n",
    "\n",
    "        if(i['tag']== tag):\n",
    "\n",
    "            result = random.choice(i['responses'])\n",
    "\n",
    "            break\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "#Creating tkinter GUI\n",
    "\n",
    "import tkinter\n",
    "\n",
    "from tkinter import *\n",
    "\n",
    "\n",
    "def process_and_send_message():\n",
    "\n",
    "    msg = MessageEntryBox.get(\"1.0\",'end-1c').strip()\n",
    "\n",
    "    MessageEntryBox.delete(\"0.0\",END)\n",
    "\n",
    "    if msg != '':\n",
    "\n",
    "        ChatDisplay.config(state=NORMAL)\n",
    "\n",
    "        ChatDisplay.insert(END, \"You: \" + msg + '\\n\\n')\n",
    "\n",
    "        ChatDisplay.config(foreground=\"#446665\", font=(\"Verdana\", 12 )) \n",
    "\n",
    "\n",
    "        ints = predict_class(msg)\n",
    "\n",
    "        res = getResponse(ints, parsed_intents)\n",
    "\n",
    "        \n",
    "\n",
    "        ChatDisplay.insert(END, \"Bot: \" + res + '\\n\\n')           \n",
    "\n",
    "\n",
    "        ChatDisplay.config(state=DISABLED)\n",
    "\n",
    "        ChatDisplay.yview(END)\n",
    "\n",
    "\n",
    "app_root = Tk()\n",
    "\n",
    "app_root.title(\"Chatbot\")\n",
    "\n",
    "app_root.geometry(\"400x500\")\n",
    "\n",
    "app_root.resizable(width=FALSE, height=FALSE)\n",
    "\n",
    "\n",
    "# Build the chat window area for displaying messages\n",
    "\n",
    "ChatDisplay = Text(app_root, bd=0, bg=\"white\", height=\"8\", width=\"50\", font=\"Arial\",)\n",
    "\n",
    "\n",
    "ChatDisplay.config(state=DISABLED)\n",
    "\n",
    "# Attach a scrollbar to the chat display window\n",
    "\n",
    "scrollbar = Scrollbar(app_root, command=ChatDisplay.yview, cursor=\"heart\")\n",
    "\n",
    "ChatDisplay['yscrollcommand'] = scrollbar.set\n",
    "\n",
    "\n",
    "#Create Button to process_and_send_message message\n",
    "\n",
    "SubmitButton = Button(app_root, font=(\"Verdana\",12,'bold'), text=\"Send\", width=\"12\", height=5,\n",
    "\n",
    "                    bd=0, bg=\"#f9a602\", activebackground=\"#3c9d9b\",fg='#000000',\n",
    "\n",
    "                    command= process_and_send_message )\n",
    "\n",
    "\n",
    "# Create a text entry box for typing messages\n",
    "\n",
    "MessageEntryBox = Text(app_root, bd=0, bg=\"white\",width=\"29\", height=\"5\", font=\"Arial\")\n",
    "\n",
    "#MessageEntryBox.bind(\"<Return>\", process_and_send_message)\n",
    "\n",
    "\n",
    "# Arrange the GUI elements on the screen\n",
    "\n",
    "scrollbar.place(x=376,y=6, height=386)\n",
    "\n",
    "ChatDisplay.place(x=6,y=6, height=386, width=370)\n",
    "\n",
    "MessageEntryBox.place(x=128, y=401, height=90, width=265)\n",
    "\n",
    "SubmitButton.place(x=6, y=401, height=90)\n",
    "\n",
    "\n",
    "app_root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7136f07e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
